{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 1, 2 & 3 Experiment\n",
    "\n",
    "This notebook walks through the major milestones of the PySpark coding challenge.\n",
    "It is intended for data scientists and engineers who wish to understand how the\n",
    "pipeline transforms raw impression and action logs into fixed‑length sequences,\n",
    "how to compute data quality metrics, and how to analyse the resulting training\n",
    "data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Day 1 – Core pipeline\n",
    "\n",
    "Day 1 focuses on building the core transformation logic.  We define strict\n",
    "Spark schemas for each input dataset (impressions, clicks, add‑to‑carts,\n",
    "previous orders) and normalise heterogeneous actions into a single table.\n",
    "Each impression row is exploded so that there is one output row per\n",
    "impression item, and we gather up to a fixed number of historical actions\n",
    "per customer–impression pair with a configurable look‑back window.  The\n",
    "resulting sequences are padded or truncated to a fixed length and stored in\n",
    "`actions` and `action_types` array columns.\n",
    "\n",
    "Below we demonstrate how to load the synthetic Day 1 data and run the\n",
    "pipeline end‑to‑end using the functions from `src.transformations`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from src import config, schema\n",
    "from src.utils import read_csv_with_schema, coerce_impressions_from_json\n",
    "from src.transformations import build_training_inputs\n",
    "\n",
    "# Start a Spark session (requires Java to be installed)\n",
    "spark = SparkSession.builder.appName('Day123Experiment').getOrCreate()\n",
    "\n",
    "# Load the small Day 1 datasets\n",
    "imp = read_csv_with_schema(spark, config.IMPRESSIONS_PATH, schema.impressions_schema)\n",
    "clk = read_csv_with_schema(spark, config.CLICKS_PATH, schema.clicks_schema)\n",
    "atc = read_csv_with_schema(spark, config.ADD_TO_CARTS_PATH, schema.add_to_carts_schema)\n",
    "ord = read_csv_with_schema(spark, config.PREVIOUS_ORDERS_PATH, schema.previous_orders_schema)\n",
    "\n",
    "# Ensure impressions are parsed if stored as JSON strings\n",
    "imp = coerce_impressions_from_json(imp)\n",
    "\n",
    "# Build the training inputs with default look‑back (365 days) and sequence length (1000)\n",
    "train_df = build_training_inputs(imp, clk, atc, ord)\n",
    "\n",
    "# Display schema and a sample row\n",
    "train_df.printSchema()\n",
    "train_df.show(3, truncate=False)\n",
    "\n",
    "# Stop the session when done\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Day 2 – CLI, Parquet export and metrics\n",
    "\n",
    "On Day 2 we extended the pipeline with a lightweight command‑line\n",
    "interface, a Parquet writer and a basic set of data quality metrics.  The\n",
    "CLI can be used to run the entire pipeline from raw CSV files to a\n",
    "Parquet dataset with optional JSON metrics output.  In code, you can call\n",
    "`calculate_dq_metrics` directly on the training DataFrame to obtain a\n",
    "dictionary of summary statistics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from src import config, schema\n",
    "from src.utils import read_csv_with_schema, coerce_impressions_from_json\n",
    "from src.transformations import build_training_inputs\n",
    "from src.metrics import calculate_dq_metrics\n",
    "\n",
    "spark = SparkSession.builder.appName('Day2Metrics').getOrCreate()\n",
    "\n",
    "# Load and process the Day 1 synthetic data\n",
    "imp = coerce_impressions_from_json(read_csv_with_schema(spark, config.IMPRESSIONS_PATH, schema.impressions_schema))\n",
    "clk = read_csv_with_schema(spark, config.CLICKS_PATH, schema.clicks_schema)\n",
    "atc = read_csv_with_schema(spark, config.ADD_TO_CARTS_PATH, schema.add_to_carts_schema)\n",
    "ord = read_csv_with_schema(spark, config.PREVIOUS_ORDERS_PATH, schema.previous_orders_schema)\n",
    "\n",
    "train_df = build_training_inputs(imp, clk, atc, ord)\n",
    "\n",
    "# Compute data quality metrics\n",
    "metrics = calculate_dq_metrics(train_df)\n",
    "print(metrics)\n",
    "\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Day 3 – Scaling and deeper analysis\n",
    "\n",
    "Day 3 introduces two enhancements:\n",
    "\n",
    "1. **Larger synthetic dataset**: The project now includes CSV files in the\n",
    "   `data/` directory with suffix `_1000` (e.g. `impressions_1000.csv`) to\n",
    "   simulate a data set with 1000 impression rows.  You can point the CLI\n",
    "   at these files or load them directly in Python for testing the scalability\n",
    "   of the pipeline.\n",
    "2. **Advanced analysis helpers**: A new module `src.analysis` exposes\n",
    "   functions for computing the distribution of historical sequence lengths\n",
    "   and the per‑position frequency of action types.  These tools help\n",
    "   diagnose sparsity and bias in the training inputs.\n",
    "\n",
    "Below we illustrate how to run the pipeline on the larger dataset and\n",
    "compute these analysis metrics.  Note that running on 1000 impressions\n",
    "may require more memory than the tiny Day 1 examples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from src import config, schema\n",
    "from src.utils import read_csv_with_schema, coerce_impressions_from_json\n",
    "from src.transformations import build_training_inputs\n",
    "from src.analysis import sequence_length_distribution, action_type_frequency_by_position\n",
    "\n",
    "spark = SparkSession.builder.appName('Day3Analysis').getOrCreate()\n",
    "\n",
    "# Load the larger datasets\n",
    "imp_big = coerce_impressions_from_json(read_csv_with_schema(spark, config.DATA_DIR + '/impressions_1000.csv', schema.impressions_schema))\n",
    "clk_big = read_csv_with_schema(spark, config.DATA_DIR + '/clicks_1000.csv', schema.clicks_schema)\n",
    "atc_big = read_csv_with_schema(spark, config.DATA_DIR + '/add_to_carts_1000.csv', schema.add_to_carts_schema)\n",
    "ord_big = read_csv_with_schema(spark, config.DATA_DIR + '/previous_orders_1000.csv', schema.previous_orders_schema)\n",
    "\n",
    "train_big = build_training_inputs(imp_big, clk_big, atc_big, ord_big)\n",
    "\n",
    "# Compute distribution of non‑zero sequence lengths\n",
    "length_dist = sequence_length_distribution(train_big)\n",
    "print('Sequence length distribution:', length_dist)\n",
    "\n",
    "# Compute action type frequencies for the first 5 positions\n",
    "type_freq = action_type_frequency_by_position(train_big, max_positions=5)\n",
    "for i, freq in enumerate(type_freq):\n",
    "    print(f'Position {i}:', freq)\n",
    "\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "The PySpark coding challenge evolves from a simple transformation\n",
    "pipeline in Day 1 to a more production‑ready workflow in Day 2, and\n",
    "finally to a scalable and analysable solution in Day 3.  Using the\n",
    "helpers provided in `src.analysis` you can gain insight into how\n",
    "customers interact with products over time and whether the model is\n",
    "seeing enough meaningful history.  Feel free to extend the analysis\n",
    "functions or adjust the look‑back window and sequence length to suit\n",
    "your particular domain.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}